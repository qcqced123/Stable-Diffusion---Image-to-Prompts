{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97deba6-8a61-47ca-983a-7fc5a3e9811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import re, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "import dataset_class.dataclass as dataset_class\n",
    "import model.metric as model_metric\n",
    "import model.metric_learning as metric_learning\n",
    "import model.model as model_arch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset_class.data_preprocessing import *\n",
    "from utils.helper import *\n",
    "from trainer.trainer_utils import *\n",
    "from model.metric import *\n",
    "from tqdm.auto import tqdm\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor, CLIPImageProcessor\n",
    "import albumentations as albumentations\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4bc6cf-f898-4ab4-b6d7-9557b61eb5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \"\"\" Pipeline Setting \"\"\"\n",
    "    train, test = True, False\n",
    "    checkpoint_dir = './saved/model'\n",
    "    resume, load_pretrained,  state_dict = False, False, '/'\n",
    "    name = 'FBP3_Base_Train_Pipeline'\n",
    "    loop = 'SD2Trainer'\n",
    "    dataset = 'SD2Dataset'  # dataset_class.dataclass.py -> FBPDataset, MPLDataset\n",
    "    model_arch = 'SD2Model'  # model.model.py -> FBPModel, MPLModel\n",
    "    style_model_arch = 'StyleExtractModel'  # model.model.py -> StyleModel\n",
    "    style_model = 'convnext_base_384_in22ft1k'\n",
    "\n",
    "    \"\"\" Common Options \"\"\"\n",
    "    wandb = True\n",
    "    optuna = False  # if you want to tune hyperparameter, set True\n",
    "    competition = 'FB3'\n",
    "    seed = 42\n",
    "    cfg_name = 'CFG'\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 4\n",
    "    \n",
    "    \"\"\" Data Options \"\"\"\n",
    "    batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c50266-5d80-4a5c-8088-958326775d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f727b225c50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, os, sys, random, json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def check_device() -> bool:\n",
    "    return torch.mps.is_available()\n",
    "\n",
    "\n",
    "def check_library(checker: bool) -> tuple:\n",
    "    \"\"\"\n",
    "    1) checker == True\n",
    "        - current device is mps\n",
    "    2) checker == False\n",
    "        - current device is cuda with cudnn\n",
    "    \"\"\"\n",
    "    if not checker:\n",
    "        _is_built = torch.backends.cudnn.is_available()\n",
    "        _is_enable = torch.backends.cudnn.enabledtorch.backends.cudnn.enabled\n",
    "        version = torch.backends.cudnn.version()\n",
    "        device = (_is_built, _is_enable, version)\n",
    "        return device\n",
    "\n",
    "\n",
    "def class2dict(cfg) -> dict:\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))\n",
    "\n",
    "\n",
    "def all_type_seed(cfg, checker: bool) -> None:\n",
    "    # python & torch seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)  # python Seed\n",
    "    random.seed(cfg.seed)  # random module Seed\n",
    "    np.random.seed(cfg.seed)  # numpy module Seed\n",
    "    torch.manual_seed(cfg.seed)  # Pytorch CPU Random Seed Maker\n",
    "\n",
    "    # device == cuda\n",
    "    if not checker:\n",
    "        torch.cuda.manual_seed(cfg.seed)  # Pytorch GPU Random Seed Maker\n",
    "        torch.cuda.manual_seed_all(cfg.seed)  # Pytorch Multi Core GPU Random Seed Maker\n",
    "        # torch.cudnn seed\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # devide == mps\n",
    "    else:\n",
    "        torch.mps.manual_seed(cfg.seed)\n",
    "\n",
    "\n",
    "def seed_worker(worker_id) -> None:\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "check_library(True)\n",
    "all_type_seed(CFG, True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449359f4-8dfc-4943-b1c7-2a3e32a85517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data_folder from csv file like as train.csv, test.csv, val.csv\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31f329f-b11e-4cef-b989-84334a3d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD2Dataset:\n",
    "    \"\"\" Image, Prompt Dataset For OpenAI CLIP Pipeline \"\"\"\n",
    "    def __init__(self, cfg, df: pd.DataFrame) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "        self.img_transform = albumentations.Compose([\n",
    "            albumentations.Resize(384, 384),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()]\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        No need to tokenize text, CLIP has its own tokenizer stage in model class (encode text)\n",
    "        return:\n",
    "            image: image for style-extractor\n",
    "            clip_image: image for CLIP\n",
    "            target: prompt for CLIP\n",
    "        \"\"\"\n",
    "        image_index = self.df.iloc[item, 0]\n",
    "        image = rasterio.open(self.df.iloc[item, 0])\n",
    "        tensor_image = image.read(resampling=Resampling.bilinear).transpose(1, 2, 0)\n",
    "        style_image = self.img_transform(image=tensor_image)['image']  # resize & normalize for style-extractor\n",
    "        return image_index, style_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bd0886-4743-4f5a-b6e1-44b99b5243c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleExtractModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class for Style-Extractor Model (EfficientNet, Convnext, ResNet, ...etc)\n",
    "    Style-Extractor Model is used for extract style feature(background) from image\n",
    "    And then, Feature will be concatenated with CLIP's Image embedding\n",
    "    This Model is used ONLY extracting embedding, just Only forward pass\n",
    "\n",
    "    In CLIP Model's Code in Huggingface, AutoProcessor do center crop to image in resizing 224x224\n",
    "    But in many prompt sentences, they have a lot of word for background called feature.\n",
    "    So we need to style-extractor for more good performance in generate prompt text\n",
    "    option:\n",
    "        style_model: efficientnet family, convnext_base, resent family\n",
    "        efficientnet: pass keyword 'blocks' to forward function\n",
    "        convnext_base: pass keyword 'stage' to forward function\n",
    "        resnet: pass keyword 'layer1 ~ layer4' to forward function\n",
    "\n",
    "    [Reference]\n",
    "    https://www.kaggle.com/code/tanreinama/style-extract-from-vgg-clip-object-extract\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.style_model = timm.create_model(\n",
    "            self.cfg.style_model,\n",
    "            pretrained=True,\n",
    "            features_only=False,  # will be drop classifier or regression head\n",
    "        )\n",
    "        self.avg = nn.AdaptiveAvgPool1d(1)\n",
    "        if 'efficientnet' in self.cfg.style_model:\n",
    "            layer_name = 'blocks'\n",
    "        elif 'convnext' in self.cfg.style_model:\n",
    "            layer_name = 'stages'\n",
    "        elif 'resnet' in self.cfg.style_model:\n",
    "            layer_name = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "        self.feature1 = self.style_model.stem + self.style_model.stages[0:1]\n",
    "        self.feature2 = self.style_model.stages[1:2]\n",
    "        self.feature3 = self.style_model.stages[2:3]\n",
    "        self.feature4 = self.style_model.stages[3:4]\n",
    "\n",
    "    @staticmethod\n",
    "    def gram_matrix(x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, h, w = x.size()\n",
    "        f = x.view(b, c, h * w)\n",
    "        g = torch.bmm(f, f.transpose(1, 2)) / (h * w)\n",
    "        return g\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        embedding1 = self.feature1(x)\n",
    "        embedding2 = self.feature2(embedding1)\n",
    "        embedding3 = self.feature3(embedding2)\n",
    "        embedding4 = self.feature4(embedding3)\n",
    "\n",
    "        g1 = self.gram_matrix(embedding1)\n",
    "        g2 = self.gram_matrix(embedding2)\n",
    "        g3 = self.gram_matrix(embedding3)\n",
    "        g4 = self.gram_matrix(embedding4)\n",
    "        g = [self.avg(g1).squeeze(2), self.avg(g2).squeeze(2), self.avg(g3).squeeze(2), self.avg(g4).squeeze(2)]\n",
    "        return torch.cat(g, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80238cef-4e1f-464d-99ce-973ac20fb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SD2Trainer:\n",
    "    \"\"\" For OpenAI CLIP Fine-Tuned Pipeline with Multiple Negative Ranking Loss, Style-Extractor \"\"\"\n",
    "    def __init__(self, cfg, generator) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.generator = generator\n",
    "        self.df = pd.read_csv('./dataset_class/final_downsample_prompt.csv')\n",
    "    def make_batch(self):\n",
    "        \"\"\" Make Batch Dataset for main train loop \"\"\"\n",
    "        # Custom Datasets\n",
    "        train_dataset = SD2Dataset(self.cfg, self.df)\n",
    "\n",
    "        # DataLoader\n",
    "        loader_train = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            shuffle=True,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=self.generator,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        return loader_train\n",
    "\n",
    "    def model_setting(self):\n",
    "        \"\"\" set train & validation options for main train loop \"\"\"\n",
    "        style_model = StyleExtractModel(self.cfg)       \n",
    "        style_model.to(self.cfg.device)\n",
    "        return style_model\n",
    "\n",
    "    # Train Function\n",
    "    def train_fn(self, loader_train, style_model):\n",
    "        \"\"\" Training Function \"\"\"\n",
    "        image_list, embedding_list = [], []\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        style_model.eval()\n",
    "        for step, (image_index, style_image) in enumerate(tqdm(loader_train)):            \n",
    "            style_image = style_image.to(self.cfg.device)  # style image to GPU\n",
    "            with torch.no_grad():\n",
    "                style_features = style_model(style_image)  # style image to style feature\n",
    "                \n",
    "            style_features = style_features.detach().cpu().numpy()\n",
    "            embedding_list.append(style_features)\n",
    "            image_list.append(image_index)\n",
    "            \n",
    "        return image_list, embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a090b0-8fb2-425b-a1cf-f9c8d692927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(cfg: any) -> None:\n",
    "    \"\"\" Base Trainer Loop Function \"\"\"\n",
    "    train_input = SD2Trainer(cfg, g)\n",
    "    loader_train = train_input.make_batch()\n",
    "    style_model = train_input.model_setting()\n",
    "    image_list, embedding_list = train_input.train_fn(\n",
    "        loader_train, style_model\n",
    "    )\n",
    "    torch.save(image_list, 'style_image_name.pth')\n",
    "    torch.save(embedding_list, 'style_embedding_name.pth')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c039cb-4dad-4555-bc29-7ac68e640204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de6c47ac2994efe98ea4a35ff09c720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/853 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/home/qcqced/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_loop(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478998a6-9d12-47e7-a489-df913accce66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
