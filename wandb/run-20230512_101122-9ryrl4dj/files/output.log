
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
[1/5] Train & Validation
/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                                                                         | 0/4127 [00:00<?, ?it/s]
Traceback (most recent call last):                                                                                            | 0/4127 [00:00<?, ?it/s]
  File "/home/qcqced/바탕화면/ML_Test/image2text/train.py", line 49, in <module>
    main('image2text_config.json', CFG)
  File "/home/qcqced/바탕화면/ML_Test/image2text/train.py", line 30, in main
    getattr(train_loop, cfg.loop)(cfg)  # init object
  File "/home/qcqced/바탕화면/ML_Test/image2text/trainer/train_loop.py", line 39, in train_loop
    train_loss, grad_norm, lr = train_input.train_fn(
  File "/home/qcqced/바탕화면/ML_Test/image2text/trainer/trainer.py", line 91, in train_fn
    for step, (images, clip_images, labels) in enumerate(tqdm(loader_train)):
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/tqdm/std.py", line 1195, in __iter__
    for obj in iterable:
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
ValueError: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/qcqced/바탕화면/ML_Test/image2text/dataset_class/dataclass.py", line 46, in __getitem__
    clip_image = self.input_processor(image)  # resize & crop for pretrained CLIP
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/models/clip/processing_clip.py", line 99, in __call__
    encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2538, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/qcqced/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2596, in _call_one
    raise ValueError(
ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).