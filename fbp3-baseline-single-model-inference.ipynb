{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch, transformers, os, sys, gc, time, random, warnings, math, json, glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cuml\n",
    "\n",
    "from cuml.svm import SVR\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:32.992186Z",
     "iopub.execute_input": "2023-04-23T14:58:32.992920Z",
     "iopub.status.idle": "2023-04-23T14:58:49.190244Z",
     "shell.execute_reply.started": "2023-04-23T14:58:32.992887Z",
     "shell.execute_reply": "2023-04-23T14:58:49.188213Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "env: TOKENIZERS_PARALLELISM=false\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Test Configuration \"\"\"\n",
    "\n",
    "# CV Score: 0.4528\n",
    "class CFG1:\n",
    "    \"\"\" Student Model Fine-Tuned with Mean Pooling, SmoothL1Loss, token_len=1536 \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/fbp3-meanpooling-max-length-1536/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss' \n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    max_len = 1536 # Original: 1536\n",
    "    batch_size = 8\n",
    "    n_folds = 5\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0\n",
    "    \n",
    "# CV Score: 0.447\n",
    "class CFG2:\n",
    "    \"\"\" Student Model Fine-Tuned with GEM Pooling, SmoothL1Loss, token_len=1536 \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/fbp3-gempooling-max-len-1536-04476/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'GEMPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    max_len = 1536 # Original: 1536\n",
    "    batch_size = 8\n",
    "    n_folds = 5\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.2\n",
    "        \n",
    "# CV Score: 0.4545\n",
    "class CFG3:\n",
    "    \"\"\" Student Model Fine-Tuned with WeightedLayerPooling, SmoothL1Loss, token_len=1024 \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/fbp3-weightedlayerpooling-04545/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'WeightedLayerPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    max_len = 1536 # Original: 1536\n",
    "    batch_size = 8\n",
    "    n_folds = 5\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 0.8\n",
    "    \n",
    "class CFG4:\n",
    "    \"\"\" Meta Pseudo Student Model with No Fine-Tuned \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/fbp3-meta-pseudo-labels-model/4538_MPL_Student_microsoft-deberta-v3-large_state_dict.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'WeightedLayerPooling'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 32\n",
    "    n_fold = 5\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 0.6\n",
    "\n",
    "    \n",
    "class CFG5:\n",
    "    \"\"\" Fine-Tuned Model from Meta Pseudo Label Student Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = [\n",
    "        '/kaggle/input/fbp3-meta-pseudo-labels-model/SWA_fold0_MeanPooling_microsoft-deberta-v3-large_state_dict.pth',\n",
    "        '/kaggle/input/fbp3-meta-pseudo-labels-model/fold1_MeanPooling_microsoft-deberta-v3-large_state_dict.pth',\n",
    "        '/kaggle/input/fbp3-meta-pseudo-labels-model/SWA_fold2_MeanPooling_microsoft-deberta-v3-large_state_dict.pth',\n",
    "        '/kaggle/input/fbp3-meta-pseudo-labels-model/fold3_MeanPooling_microsoft-deberta-v3-large_state_dict.pth',\n",
    "        '/kaggle/input/fbp3-meta-pseudo-labels-model/fold4_MeanPooling_microsoft-deberta-v3-large_state_dict.pth'\n",
    "    ]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 32\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 0.8\n",
    "\n",
    "class CFG6:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/0925-deberta-v3-large-unscale/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.2\n",
    "    \n",
    "class CFG7:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/0926-deberta-v3-large-unscale/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.2\n",
    "    \n",
    "class CFG8:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/0927-deberta-v3-large-unscale/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.2\n",
    "    \n",
    "class CFG9:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/0911-deberta-v3-large/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0\n",
    "    \n",
    "class CFG10:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "    model_list = glob.glob('/kaggle/input/0914-deberta-v3-large-fgm/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0\n",
    "    \n",
    "class CFG11:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-base'\n",
    "    model_list = glob.glob('/kaggle/input/0911-deberta-v3-base/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0\n",
    "    \n",
    "class CFG12:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v3-base'\n",
    "    model_list = glob.glob('/kaggle/input/0913-deberta-v3-base-fgm/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0\n",
    "    \n",
    "class CFG13:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v2-xlarge'\n",
    "    model_list = glob.glob('/kaggle/input/0911-deberta-v2-xlarge/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0\n",
    "    \n",
    "class CFG14:\n",
    "    \"\"\" No MPL Fine-Tuned Model \"\"\"\n",
    "    model = '/kaggle/input/huggingface-automodel-save/deberta-v2-xlarge'\n",
    "    model_list = glob.glob('/kaggle/input/0919-deberta-v2-xlarge/*.pth')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pooling = 'MeanPooling'  # mean, attention, max, weightedlayer, concat, conv1d, lstm\n",
    "    loss_fn = 'SmoothL1Loss'\n",
    "    seed = 42\n",
    "    n_gpu = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_id = 0\n",
    "    num_workers = 0\n",
    "    gradient_checkpointing = False\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    n_fold = 10\n",
    "    num_freeze = 4\n",
    "    num_reinit = 2\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    weight = 1.0"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:49.192818Z",
     "iopub.execute_input": "2023-04-23T14:58:49.193552Z",
     "iopub.status.idle": "2023-04-23T14:58:52.920050Z",
     "shell.execute_reply.started": "2023-04-23T14:58:49.193486Z",
     "shell.execute_reply": "2023-04-23T14:58:52.918684Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Helper Function \"\"\"\n",
    "\n",
    "def check_device() -> bool:\n",
    "    return torch.mps.is_available()\n",
    "\n",
    "def check_library(checker: bool) -> tuple:\n",
    "    \"\"\"\n",
    "    1) checker == True\n",
    "        - current device is mps\n",
    "    2) checker == False\n",
    "        - current device is cuda with cudnn\n",
    "    \"\"\"\n",
    "    if not checker:\n",
    "        _is_built = torch.backends.cudnn.is_available()\n",
    "        _is_enable = torch.backends.cudnn.enabledtorch.backends.cudnn.enabled\n",
    "        version = torch.backends.cudnn.version()\n",
    "        device = (_is_built, _is_enable, version)\n",
    "        return device\n",
    "\n",
    "def class2dict(cfg) -> dict:\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))\n",
    "\n",
    "\n",
    "def all_type_seed(cfg, checker: bool) -> None:\n",
    "    # python & torch seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)  # python Seed\n",
    "    random.seed(cfg.seed)  # random module Seed\n",
    "    np.random.seed(cfg.seed)  # numpy module Seed\n",
    "    torch.manual_seed(cfg.seed)  # Pytorch CPU Random Seed Maker\n",
    "\n",
    "    # device == cuda\n",
    "    if not checker:\n",
    "        torch.cuda.manual_seed(cfg.seed)  # Pytorch GPU Random Seed Maker\n",
    "        torch.cuda.manual_seed_all(cfg.seed)  # Pytorch Multi Core GPU Random Seed Maker\n",
    "        # torch.cudnn seed\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "def seed_worker(worker_id) -> None:\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "check_library(True)\n",
    "all_type_seed(CFG3, True)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(CFG3.seed)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:52.922161Z",
     "iopub.execute_input": "2023-04-23T14:58:52.922592Z",
     "iopub.status.idle": "2023-04-23T14:58:52.941677Z",
     "shell.execute_reply.started": "2023-04-23T14:58:52.922546Z",
     "shell.execute_reply": "2023-04-23T14:58:52.940370Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torch._C.Generator at 0x71256bcdfd30>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Data Utils \"\"\"\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data_folder from csv file like as train.csv, test.csv, val.csv\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "def text_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For FB3 Text Data\n",
    "    FB3 Text data_folder has '\\n\\n', meaning that separate paragraphs are separated by '\\n\\n'\n",
    "    DeBERTa does not handle '\\n\\n' well, so we need to change them into token '[PARAGRAPH]'\n",
    "    \"\"\"\n",
    "    text_list = df['full_text'].values.tolist()\n",
    "    text_list = [text.replace('\\n\\n', '[PARAGRAPH] ') for text in text_list]\n",
    "    df['full_text'] = text_list\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_word_normalizer():\n",
    "    \"\"\"\n",
    "    Create a function that normalizes a word.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def normalize(word):\n",
    "        w = word.lower()\n",
    "        w = lemmatizer.lemmatize(w)\n",
    "        w = ps.stem(w)\n",
    "        return w\n",
    "    return normalize\n",
    "\n",
    "def __normalize_words(titles: list) -> list:\n",
    "    \"\"\"\n",
    "    Normalize a list of words\n",
    "    1) Remove stop words\n",
    "    2) Apply Porter Stemmer, Lemmatizer\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    normalizer = create_word_normalizer()\n",
    "    titles = [normalizer(t) for t in titles if t not in stop_words]\n",
    "    return titles\n",
    "\n",
    "def normalize_words(words: np.ndarray, unique=True) -> list:\n",
    "    \"\"\"\n",
    "    Normalize a list of words\n",
    "    1) Apply __normalize_word function\n",
    "    2) Apply Regular Expression to remove special characters\n",
    "    \"\"\"\n",
    "    if type(words) is str:\n",
    "        words = [words]\n",
    "    sep_re = r'[\\s\\(\\){}\\[\\];,\\.]+'\n",
    "    num_re = r'\\d'\n",
    "    words = re.split(sep_re, ' '.join(words).lower())\n",
    "    words = [w for w in words if len(w) >= 3 and not re.match(num_re, w)]\n",
    "    if unique:\n",
    "        words = list(set(words))\n",
    "        words = set(__normalize_words(words))\n",
    "    else:\n",
    "        words = __normalize_words(words)\n",
    "    return words\n",
    "\n",
    "def collate(inputs):\n",
    "    \"\"\" Descending sort inputs by length of sequence \"\"\"\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:, :mask_len]\n",
    "    return inputs\n",
    "\n",
    "def get_name(cfg) -> str:\n",
    "    \"\"\" get name of model \"\"\"\n",
    "    try:\n",
    "        name = cfg.model.replace('/', '-')\n",
    "    except ValueError:\n",
    "        name = cfg.model\n",
    "    return name"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:52.945082Z",
     "iopub.execute_input": "2023-04-23T14:58:52.945478Z",
     "iopub.status.idle": "2023-04-23T14:58:52.959174Z",
     "shell.execute_reply.started": "2023-04-23T14:58:52.945442Z",
     "shell.execute_reply": "2023-04-23T14:58:52.958034Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Pooling & Model Utils \"\"\"\n",
    "\n",
    "# Mean Pooling\n",
    "class GEMPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized Mean Pooling for Natural Language Processing\n",
    "    This class version of GEMPooling for NLP, Transfer from Computer Vision Task Code\n",
    "    Mean Pooling <= GEMPooling <= Max Pooling\n",
    "    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token\n",
    "\n",
    "    [Reference]\n",
    "    https://paperswithcode.com/method/generalized-mean-pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_cfg):\n",
    "        super(GEMPooling, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(last_hidden_state, attention_mask, p: int = 4) -> Tensor:\n",
    "        \"\"\"\n",
    "        1) Expand Attention Mask from [batch_size, max_len] to [batch_size, max_len, hidden_size]\n",
    "        2) Sum Embeddings along max_len axis so now we have [batch_size, hidden_size]\n",
    "        3) Sum Mask along max_len axis, This is done so that we can ignore padding tokens\n",
    "        4) Average\n",
    "        \"\"\"\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(\n",
    "            torch.pow(last_hidden_state * input_mask_expanded, p), 1\n",
    "        )\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        gem_embeddings = torch.pow(sum_embeddings / sum_mask, 1/p)\n",
    "        return gem_embeddings\n",
    "\n",
    "\n",
    "# WeightedLayerPooling: Use Intermediate Layer's Embedding\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    For Weighted Layer Pooling Class\n",
    "    In Original Paper, they use [CLS] token for classification task.\n",
    "    But in common sense, Mean Pooling more good performance than CLS token Pooling\n",
    "    So, we append Last part of this Pooling Method, Using CLS Token then Mean Pooling Embedding\n",
    "    Args:\n",
    "        auto_cfg: AutoConfig from model class member variable\n",
    "        layer_start: start layer for pooling, default 4\n",
    "        layer_weights: layer weights for pooling, default None\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_cfg, layer_start: int = 12, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = auto_cfg.num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (self.num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states, attention_mask) -> Tensor:\n",
    "        all_layer_embedding = torch.stack(list(all_hidden_states), dim=0)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(weighted_average.size()).float()\n",
    "        sum_embeddings = torch.sum(weighted_average * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)  # if lower than thres, replace value to threshold (parameter min)\n",
    "        weighted_mean_embeddings = sum_embeddings / sum_mask\n",
    "        return weighted_mean_embeddings\n",
    "\n",
    "# Attention pooling\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    [Reference]\n",
    "    <A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING>\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_cfg):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "           nn.Linear(auto_cfg.hidden_size, auto_cfg.hidden_size),\n",
    "           nn.LayerNorm(auto_cfg.hidden_size),\n",
    "           nn.GELU(),\n",
    "           nn.Linear(auto_cfg.hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask) -> Tensor:\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask == 0] = float('-inf')\n",
    "        w = torch.softmax(w, 1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "\n",
    "# Mean Pooling\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self, auto_cfg):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(last_hidden_state, attention_mask) -> Tensor:\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "# Max Pooling\n",
    "class MaxPooling(nn.Module):\n",
    "    def __init__(self, auto_cfg):\n",
    "        super(MaxPooling, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = -1e4\n",
    "        max_embeddings, _ = torch.max(embeddings, dim=1)\n",
    "        return max_embeddings\n",
    "\n",
    "\n",
    "# Min Pooling\n",
    "class MinPooling(nn.Module):\n",
    "    def __init__(self, auto_cfg):\n",
    "        super(MinPooling, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state.clone()\n",
    "        embeddings[input_mask_expanded == 0] = 1e-4\n",
    "        min_embeddings, _ = torch.min(embeddings, dim=1)\n",
    "        return min_embeddings\n",
    "\n",
    "\n",
    "# Convolution Pooling\n",
    "class ConvPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    for filtering unwanted feature such as Toxicity Text, Negative Comment...etc\n",
    "    kernel_size: similar as window size\n",
    "\n",
    "    [Reference]\n",
    "    https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_size: int, kernel_size: int, padding_size: int):\n",
    "        super().__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding_size = padding_size\n",
    "        self.convolution = nn.Sequential(\n",
    "            nn.Conv1d(self.feature_size, 256, kernel_size=self.kernel_size, padding=self.padding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 1, kernel_size=kernel_size, padding=padding_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state: Tensor) -> Tensor:\n",
    "        embeddings = last_hidden_state.permute(0, 2, 1) # (batch_size, feature_size, seq_len)\n",
    "        logit, _ = torch.max(self.convolution(embeddings), 2)\n",
    "        return logit\n",
    "\n",
    "\n",
    "# LSTM Pooling\n",
    "class LSTMPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    [Reference]\n",
    "    https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n",
    "    \"\"\"\n",
    "    def __int__(self, num_layers: int, hidden_size: int, hidden_dim_lstm):\n",
    "        super().__init__()\n",
    "        self.num_hidden_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.hidden_size,\n",
    "            self.hidden_dim_lstm,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, all_hidden_states: list) -> Tensor:\n",
    "        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\\\n",
    "                                    for layer_i in range(1, self.num_hidden_layers)], dim=1)\n",
    "        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n",
    "        out, _ = self.lstm(hidden_states, None)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "def freeze(module) -> None:\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "\n",
    "    [Example]\n",
    "    freezing embeddings and first 2 layers of encoder\n",
    "    1) freeze(model.embeddings)\n",
    "    2) freeze(model.encoder.layer[:2])\n",
    "    \"\"\"\n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "def get_freeze_parameters(module) -> list:\n",
    "    \"\"\"\n",
    "    Returns names of freezed parameters of the given module.\n",
    "\n",
    "    [Example]\n",
    "    freezed_parameters = get_freezed_parameters(model)\n",
    "    \"\"\"\n",
    "    freezed_parameters = []\n",
    "    for name, parameter in module.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            freezed_parameters.append(name)\n",
    "\n",
    "    return freezed_parameters\n",
    "\n",
    "def init_weights(auto_cfg, module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes weights of the given module.\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module.weight.data.normal_(mean=0.0, std=auto_cfg.initializer_range)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=auto_cfg.initializer_range)\n",
    "        if module.padding_idx is not None:\n",
    "            module.weight.data[module.padding_idx].zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "\n",
    "def reinit_topk(model, num_layers) -> None:\n",
    "    \"\"\"\n",
    "    Re-initialize the last-k transformer Encoder layers.\n",
    "    Encoder Layer: Embedding, Attention Head, LayerNorm, Feed Forward\n",
    "    Args:\n",
    "        model: The target transformer model.\n",
    "        num_layers: The number of layers to be re-initialized.\n",
    "    \"\"\"\n",
    "    model.encoder.layer[-num_layers:].apply(model._init_weights)\n",
    "    \n",
    "def postprocess(pseudo_label):\n",
    "    \"\"\" for post processing to teacher model's prediction(pseudo label) \"\"\"\n",
    "    label_dict = torch.arange(1, 5.5, 0.5)\n",
    "    pseudo_label.squeeze()\n",
    "    for instance in pseudo_label:\n",
    "        for idx in range(len(instance)):\n",
    "            instance[idx] = label_dict[(torch.abs(label_dict - instance[idx]) == min(torch.abs(label_dict - instance[idx]))).nonzero(as_tuple=False)]\n",
    "    return pseudo_label\n",
    "\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction=reduction)\n",
    "        self.eps = eps # If MSE == 0, We need eps\n",
    "\n",
    "    def forward(self, yhat, y) -> Tensor:\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    # num_scored => setting your number of metrics\n",
    "    def __init__(self, reduction, num_scored=6):\n",
    "        super().__init__()\n",
    "        self.RMSE = RMSELoss(reduction=reduction)\n",
    "        self.num_scored = num_scored\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        score = 0\n",
    "        for i in range(self.num_scored):\n",
    "            score = score + (self.RMSE(yhat[:, i], y[:, i]) / self.num_scored)\n",
    "        return score"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:52.960919Z",
     "iopub.execute_input": "2023-04-23T14:58:52.962350Z",
     "iopub.status.idle": "2023-04-23T14:58:53.000773Z",
     "shell.execute_reply.started": "2023-04-23T14:58:52.962314Z",
     "shell.execute_reply": "2023-04-23T14:58:52.999676Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Test Dataset Class \"\"\"\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\" For Inference Dataset Class \"\"\"\n",
    "    def __init__(self, cfg, df):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.df = df\n",
    "\n",
    "    def tokenizing(self, text):\n",
    "        inputs = self.cfg.tokenizer(\n",
    "            text,\n",
    "            max_length=self.cfg.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v)\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizing(self.df.iloc[idx, 1])\n",
    "        return inputs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.002295Z",
     "iopub.execute_input": "2023-04-23T14:58:53.002652Z",
     "iopub.status.idle": "2023-04-23T14:58:53.017256Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.002613Z",
     "shell.execute_reply": "2023-04-23T14:58:53.016197Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TestModel(nn.Module):\n",
    "    \"\"\" Model class for Baseline Pipeline \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.auto_cfg = AutoConfig.from_pretrained(\n",
    "            cfg.model,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            cfg.model,\n",
    "            config=self.auto_cfg\n",
    "        )\n",
    "        self.fc = nn.Linear(self.auto_cfg.hidden_size, 6)\n",
    "        \n",
    "        if cfg.pooling == 'MeanPooling':\n",
    "            self.pooling = MeanPooling(self.auto_cfg)\n",
    "        elif cfg.pooling == 'AttentionPooling':\n",
    "            self.pooling = AttentionPooling(self.auto_cfg)\n",
    "        elif cfg.pooling == 'WeightedLayerPooling':\n",
    "            self.pooling = WeightedLayerPooling(self.auto_cfg)\n",
    "        elif cfg.pooling == 'GEMPooling':\n",
    "            self.pooling = GEMPooling(self.auto_cfg)\n",
    "        self._init_weights(self.fc)\n",
    "        reinit_topk(self.model, cfg.num_reinit)\n",
    "        freeze(self.model)\n",
    "\n",
    "    def _init_weights(self, module) -> None:\n",
    "        \"\"\" over-ride initializes weights of the given module function (+initializes LayerNorm) \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.auto_cfg.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            \"\"\" reference from torch.nn.Layernorm with elementwise_affine=True \"\"\"\n",
    "            module.weight.data.fill_(1.0)\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def feature(self, inputs: dict):\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs: dict) -> Tensor:\n",
    "        outputs = self.feature(inputs)\n",
    "        feature = outputs.last_hidden_state\n",
    "        if self.cfg.pooling == 'WeightedLayerPooling':\n",
    "            feature = outputs.hidden_states\n",
    "        embedding = self.pooling(feature, inputs['attention_mask'])\n",
    "        logit = self.fc(embedding)\n",
    "        return logit\n",
    "\n",
    "class NoMPLModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "        else:\n",
    "            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling(self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.018976Z",
     "iopub.execute_input": "2023-04-23T14:58:53.019416Z",
     "iopub.status.idle": "2023-04-23T14:58:53.041372Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.019379Z",
     "shell.execute_reply": "2023-04-23T14:58:53.040337Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Make Test Loop's Input \"\"\"\n",
    "\n",
    "class FBPTest:\n",
    "    def __init__(self, cfg, generator):\n",
    "        self.cfg = cfg\n",
    "        self.generator = generator\n",
    "        self.df = text_preprocess(load_data('/kaggle/input/feedback-prize-english-language-learning/test.csv'))\n",
    "        self.tokenizer = self.cfg.tokenizer\n",
    "\n",
    "    def make_batch(self):\n",
    "        test = self.df.reset_index(drop=True)\n",
    "\n",
    "        # Custom Datasets\n",
    "        test_dataset = TestDataset(self.cfg, self.tokenizer, test)\n",
    "\n",
    "        # DataLoader\n",
    "        loader_test = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=self.generator,\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        return loader_test\n",
    "\n",
    "    def model_setting(self, model_path):\n",
    "        model = TestModel(self.cfg)\n",
    "        model.load_state_dict(\n",
    "            torch.load(model_path),\n",
    "            strict=False\n",
    "        )\n",
    "        model.to(self.cfg.device)\n",
    "        return model\n",
    "    \n",
    "    def test_fn(self, loader_test, model):\n",
    "        \"\"\" Test Function \"\"\"\n",
    "        model.eval()\n",
    "        y_preds = []\n",
    "        for step, inputs in enumerate(tqdm(loader_test)):\n",
    "            inputs = collate(inputs)\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(self.cfg.device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs)\n",
    "            y_preds.append(preds.to('cpu').numpy())\n",
    "        return y_preds"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.042962Z",
     "iopub.execute_input": "2023-04-23T14:58:53.043341Z",
     "iopub.status.idle": "2023-04-23T14:58:53.058134Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.043303Z",
     "shell.execute_reply": "2023-04-23T14:58:53.056995Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Test Loop \"\"\"\n",
    "def test_loop(cfg: any, model_path) -> pd.DataFrame:\n",
    "    test_input = FBPTest(cfg, g)  # init object\n",
    "    loader_test = test_input.make_batch()\n",
    "    model = test_input.model_setting(model_path)\n",
    "    y_preds = test_input.test_fn(loader_test, model)\n",
    "    del test_input, loader_test, model\n",
    "    gc.collect()\n",
    "    return y_preds"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.059587Z",
     "iopub.execute_input": "2023-04-23T14:58:53.060202Z",
     "iopub.status.idle": "2023-04-23T14:58:53.073369Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.060164Z",
     "shell.execute_reply": "2023-04-23T14:58:53.072299Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.078056Z",
     "iopub.execute_input": "2023-04-23T14:58:53.078549Z",
     "iopub.status.idle": "2023-04-23T14:58:53.087454Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.078522Z",
     "shell.execute_reply": "2023-04-23T14:58:53.086410Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# \"\"\" Let's Inference \"\"\"\n",
    "# sample_submission_df = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv')\n",
    "# # test_df = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n",
    "# submission_df = sample_submission_df.copy()\n",
    "# # final_predictions = np.zeros((len(test_df), len(test_df.iloc[0,1:])))\n",
    "# final_predictions = 0\n",
    "# for idx, cfg in enumerate(tqdm(cfg_list)):\n",
    "# #     cfg_predictions = np.zeros((len(test_df), len(test_df.iloc[0,1:])))\n",
    "#     cfg_predictions = 0\n",
    "#     for model_path in tqdm(cfg.model_list):\n",
    "#         predictions = []\n",
    "#         predictions.append(test_loop(cfg, model_path))\n",
    "#         predictions = np.array(predictions).squeeze() #.mean(axis=0)\n",
    "# #         cfg_predictions += np.add(cfg_predictions, predictions)\n",
    "#         cfg_predictions += predictions\n",
    "#     cfg_predictions = cfg_predictions * 1/cfg.n_folds\n",
    "#     final_predictions += cfg_predictions    \n",
    "# final_predictions = final_predictions * 1/len(cfg_list)\n",
    "# submission_df[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']] = final_predictions\n",
    "# submission_df.reset_index(drop=True, inplace=True)\n",
    "# submission_df.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.090921Z",
     "iopub.execute_input": "2023-04-23T14:58:53.091210Z",
     "iopub.status.idle": "2023-04-23T14:58:53.103119Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.091184Z",
     "shell.execute_reply": "2023-04-23T14:58:53.102219Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cfg_list = [CFG1, CFG2, CFG3, CFG4, CFG5, CFG6, CFG7, CFG8, CFG9, CFG10, CFG11, CFG12, CFG13, CFG14]\n",
    "\n",
    "for _idx, CFG in enumerate(tqdm(cfg_list)):\n",
    "    test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
    "    submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n",
    "    # sort by length to speed up inference\n",
    "    test['tokenize_length'] = [len(CFG.tokenizer(text)['input_ids']) for text in test['full_text'].values]\n",
    "    test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    test_dataset = TestDataset(CFG, test)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    predictions = []\n",
    "    for fold in tqdm(CFG.model_list):\n",
    "        if _idx < 5:\n",
    "            model = TestModel(CFG)\n",
    "            state = torch.load(fold, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state)\n",
    "        else:\n",
    "            model = NoMPLModel(CFG)            \n",
    "            state = torch.load(fold, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "        prediction = inference_fn(test_loader, model, CFG.device)\n",
    "        predictions.append(prediction)\n",
    "        del model, state, prediction; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "    test[CFG.target_cols] = predictions\n",
    "    submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n",
    "    display(submission.head())\n",
    "    submission[['text_id'] + CFG.target_cols].to_csv(f'submission_{_idx + 1}.csv', index=False)\n",
    "    del test, submission, predictions, test_dataset, test_loader; gc.collect()\n",
    "    torch.cuda.empty_cache() "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T14:58:53.105011Z",
     "iopub.execute_input": "2023-04-23T14:58:53.106082Z",
     "iopub.status.idle": "2023-04-23T15:46:37.432601Z",
     "shell.execute_reply.started": "2023-04-23T14:58:53.106045Z",
     "shell.execute_reply": "2023-04-23T15:46:37.430208Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/14 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93f8d6769d6d4cbdb76d97fe2c39b666"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d31be808dd86468bbeb7fc03006e722e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f462f1f84f44b5994534bda0fc19604"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62efc39a24894ce89d7de83111d85e1e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "792120cd1afd45e3a620271e8becf17f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf367e72faf74400ad20f0b80bf0d422"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c137d96afff46198df24cedd40a9c93"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.852191  2.814463    3.124552     2.925644  2.610543   \n1  000BAD50D026  2.615537  2.498952    2.702053     2.355212  2.077511   \n2  00367BB2546B  3.634399  3.531759    3.656657     3.721699  3.583319   \n\n   conventions  \n0     2.707259  \n1     2.603427  \n2     3.433978  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.852191</td>\n      <td>2.814463</td>\n      <td>3.124552</td>\n      <td>2.925644</td>\n      <td>2.610543</td>\n      <td>2.707259</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.615537</td>\n      <td>2.498952</td>\n      <td>2.702053</td>\n      <td>2.355212</td>\n      <td>2.077511</td>\n      <td>2.603427</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.634399</td>\n      <td>3.531759</td>\n      <td>3.656657</td>\n      <td>3.721699</td>\n      <td>3.583319</td>\n      <td>3.433978</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc03f37b3f8a4385bbde3ae5461a7272"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "519a14b975c4407fa34c657b9ff6b6b5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5338ab8928ad445da03f81609bf27191"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fc048c85ff443d6b0f8bfb69aa68c3a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e1ce69564bf4ae9a604cabbe412738b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d32e1f4dc8fd41a6a1abbd07af80c8f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.815866  2.753490    3.090136     2.930807  2.608538   \n1  000BAD50D026  2.677987  2.433232    2.677811     2.417471  2.154791   \n2  00367BB2546B  3.579234  3.460128    3.610293     3.676425  3.518150   \n\n   conventions  \n0     2.639577  \n1     2.610936  \n2     3.454332  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.815866</td>\n      <td>2.753490</td>\n      <td>3.090136</td>\n      <td>2.930807</td>\n      <td>2.608538</td>\n      <td>2.639577</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.677987</td>\n      <td>2.433232</td>\n      <td>2.677811</td>\n      <td>2.417471</td>\n      <td>2.154791</td>\n      <td>2.610936</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.579234</td>\n      <td>3.460128</td>\n      <td>3.610293</td>\n      <td>3.676425</td>\n      <td>3.518150</td>\n      <td>3.454332</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67d18776e8104d3397081f4db828d189"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3a49f7445d34316840d6a65282d7e57"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1ca79f000bd49ae9d70e5714ba7ee5b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7c8f61e75774e03b104fa05a21500b2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8b131b8f31d4c10a593b33f016d1292"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1abdc458230449287b6093d392d93a2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.882582  2.747907    3.180613     2.900436  2.666481   \n1  000BAD50D026  2.682862  2.498912    2.758105     2.458223  2.309281   \n2  00367BB2546B  3.507451  3.397943    3.559095     3.514899  3.433376   \n\n   conventions  \n0     2.673689  \n1     2.580015  \n2     3.392580  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.882582</td>\n      <td>2.747907</td>\n      <td>3.180613</td>\n      <td>2.900436</td>\n      <td>2.666481</td>\n      <td>2.673689</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.682862</td>\n      <td>2.498912</td>\n      <td>2.758105</td>\n      <td>2.458223</td>\n      <td>2.309281</td>\n      <td>2.580015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.507451</td>\n      <td>3.397943</td>\n      <td>3.559095</td>\n      <td>3.514899</td>\n      <td>3.433376</td>\n      <td>3.392580</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0256b7d93a16457d8f93e1f341db385e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71bc8fbe565d438e9fa708b7accd7dfc"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.871905  2.812961    3.049408     2.916902  2.711879   \n1  000BAD50D026  2.695753  2.542526    2.783838     2.460367  2.182987   \n2  00367BB2546B  3.752302  3.609422    3.701724     3.732062  3.624899   \n\n   conventions  \n0     2.707875  \n1     2.660824  \n2     3.474257  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.871905</td>\n      <td>2.812961</td>\n      <td>3.049408</td>\n      <td>2.916902</td>\n      <td>2.711879</td>\n      <td>2.707875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.695753</td>\n      <td>2.542526</td>\n      <td>2.783838</td>\n      <td>2.460367</td>\n      <td>2.182987</td>\n      <td>2.660824</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.752302</td>\n      <td>3.609422</td>\n      <td>3.701724</td>\n      <td>3.732062</td>\n      <td>3.624899</td>\n      <td>3.474257</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1dd3cafd9414804b8d614712d331c9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5be694fd20114512854d8c1820c77bdf"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfd81663acbf430897eeb7aafbb9380f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b5b2c34e1d14e0a985d654c0732f93d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "955f101412714738bc5c39cb56ff8f94"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bbec9ef4742480fa0963ed51d5b03f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.918238  2.794642    3.080143     2.943886  2.754395   \n1  000BAD50D026  2.750212  2.524597    2.781210     2.435717  2.199605   \n2  00367BB2546B  3.737241  3.521809    3.672317     3.703747  3.570585   \n\n   conventions  \n0     2.717892  \n1     2.661438  \n2     3.436521  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.918238</td>\n      <td>2.794642</td>\n      <td>3.080143</td>\n      <td>2.943886</td>\n      <td>2.754395</td>\n      <td>2.717892</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.750212</td>\n      <td>2.524597</td>\n      <td>2.781210</td>\n      <td>2.435717</td>\n      <td>2.199605</td>\n      <td>2.661438</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.737241</td>\n      <td>3.521809</td>\n      <td>3.672317</td>\n      <td>3.703747</td>\n      <td>3.570585</td>\n      <td>3.436521</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83c2d8759d704e2989f3bad18cb53d5e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "870313c5aa5f4393be881dcbe519f860"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62bb46899e6049bc8eb12130ed355856"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f99b1a1ab6ea460f8994505b24d0528a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3921cace90f543edb1988813251a6c4f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cedf2f869374f5f93e365b49a01ff9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "643ae8d993104355a1a80415f6fe45f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "808f461827dd4bb890936609ec4e655b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f7b2def7b6d479e9465111d036c14da"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "904fdad8224249daac461c7a494412bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1f14fcf34e845498b81f667b17f077a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.808707  2.722469    2.962899     2.831994  2.622270   \n1  000BAD50D026  2.683296  2.532859    2.752749     2.453870  2.230445   \n2  00367BB2546B  3.403564  3.262930    3.451626     3.400070  3.211327   \n\n   conventions  \n0     2.624713  \n1     2.712891  \n2     3.166316  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.808707</td>\n      <td>2.722469</td>\n      <td>2.962899</td>\n      <td>2.831994</td>\n      <td>2.622270</td>\n      <td>2.624713</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.683296</td>\n      <td>2.532859</td>\n      <td>2.752749</td>\n      <td>2.453870</td>\n      <td>2.230445</td>\n      <td>2.712891</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.403564</td>\n      <td>3.262930</td>\n      <td>3.451626</td>\n      <td>3.400070</td>\n      <td>3.211327</td>\n      <td>3.166316</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35f537b98e8542bcb7a9706068020e8a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55aa31e5fa044301bfcf590e9d482d94"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02a87ba649f64116acffd9e9dfd3f9fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee385190f300469d84f3d2fa04c01228"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ff8f0ccdb9246a69457633ff30cae9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e6af3cb5b8f491383bee5d5de46641f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bc1102891f84b34a368a448ce2d876f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c29c26bb1c0c4f0fb5f2af337d2c2d12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f99fbb0c659846a196ddb8d88571f1e0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88ce5d8ba2124245ac241e8c7366e6c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "267012855e624836b8e426f892c1e10e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.905139  2.752544    3.061124     2.887842  2.655701   \n1  000BAD50D026  2.618809  2.476544    2.718756     2.394442  2.144190   \n2  00367BB2546B  3.462291  3.321768    3.590484     3.530361  3.362374   \n\n   conventions  \n0     2.661439  \n1     2.609311  \n2     3.153605  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.905139</td>\n      <td>2.752544</td>\n      <td>3.061124</td>\n      <td>2.887842</td>\n      <td>2.655701</td>\n      <td>2.661439</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.618809</td>\n      <td>2.476544</td>\n      <td>2.718756</td>\n      <td>2.394442</td>\n      <td>2.144190</td>\n      <td>2.609311</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.462291</td>\n      <td>3.321768</td>\n      <td>3.590484</td>\n      <td>3.530361</td>\n      <td>3.362374</td>\n      <td>3.153605</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df4c13b2196e4cdab46b906f0f9177ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eff45f5ecfdd4835b09ff840b04445a1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40d8d6e476c148528e2af1687e3d70ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fa1b5341c9a4755aadc60a9b86cca65"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1c16462fe8f47cd9ed3c64005200d86"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c278c5cbbafc4000919740caccad8be3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "237398f3c6dc451c84357b09cad65e01"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81746dc33f0647a092ad5f2118f87c63"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a3e561ec4a34d3085b284eb6fccb04c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "deb05d41d9fa42268f79faf2ae771ab3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6fed96dede6431d9af6f4a061989ebc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.849788  2.700627    3.027683     2.888731  2.669844   \n1  000BAD50D026  2.682458  2.451743    2.726842     2.408920  2.121978   \n2  00367BB2546B  3.478536  3.295372    3.531762     3.515471  3.339186   \n\n   conventions  \n0     2.622761  \n1     2.678275  \n2     3.160689  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.849788</td>\n      <td>2.700627</td>\n      <td>3.027683</td>\n      <td>2.888731</td>\n      <td>2.669844</td>\n      <td>2.622761</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.682458</td>\n      <td>2.451743</td>\n      <td>2.726842</td>\n      <td>2.408920</td>\n      <td>2.121978</td>\n      <td>2.678275</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.478536</td>\n      <td>3.295372</td>\n      <td>3.531762</td>\n      <td>3.515471</td>\n      <td>3.339186</td>\n      <td>3.160689</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29d2f58a29294d009ac6efcfa2be49f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a05f0181895241a48462d4c31dcc99de"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "765a36f5726f4ebe9c88f41666f4c588"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87ec6b4eeb28400fb802b273d36f1957"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bef3effc5aea404c91121e73514b6f4b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "040009f9bcb7493da11fb66d512e5e35"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29513d3058354e4db7eb2bf996233ebd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e26459246702431e8f737476249d1686"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bdb5fd04aaf442ebe6ba16758edc676"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3106648dce546ba9e4b5e72021f7f02"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d6bfee4d579441f889ec39de40c9ff2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.914353  2.736979    3.041348     2.928318  2.723809   \n1  000BAD50D026  2.683547  2.490253    2.732694     2.434463  2.183360   \n2  00367BB2546B  3.506020  3.289566    3.546155     3.524339  3.388935   \n\n   conventions  \n0     2.654375  \n1     2.691896  \n2     3.196234  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.914353</td>\n      <td>2.736979</td>\n      <td>3.041348</td>\n      <td>2.928318</td>\n      <td>2.723809</td>\n      <td>2.654375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.683547</td>\n      <td>2.490253</td>\n      <td>2.732694</td>\n      <td>2.434463</td>\n      <td>2.183360</td>\n      <td>2.691896</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.506020</td>\n      <td>3.289566</td>\n      <td>3.546155</td>\n      <td>3.524339</td>\n      <td>3.388935</td>\n      <td>3.196234</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ab4ab34f8d140648fc4300f609b9d15"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0476c31aa6164422ac8b32fb1a860dc8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad4557d257f94dbca9b0120b6296acdf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88d8dee9dc7148e5a95a47f8cbccf71d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3412bfbda8f74708b0a96b4a3fbac66d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0c10c1bd3a64782b2e89a05b6d07ce5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2d26f297bc1476d9c894e90412fcfe6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0667efdab114f829bae2f1b57aa8b13"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eabba9419783415ca315d908d0317576"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c83cdd19db1548428b84409a84b6c51a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6fedce5e6cd4524a6ce7ac4bffc9da8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.930743  2.749926    3.052190     2.940608  2.714919   \n1  000BAD50D026  2.673043  2.483314    2.719683     2.422419  2.165154   \n2  00367BB2546B  3.512811  3.306587    3.550534     3.526885  3.379601   \n\n   conventions  \n0     2.670341  \n1     2.679630  \n2     3.199553  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.930743</td>\n      <td>2.749926</td>\n      <td>3.052190</td>\n      <td>2.940608</td>\n      <td>2.714919</td>\n      <td>2.670341</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.673043</td>\n      <td>2.483314</td>\n      <td>2.719683</td>\n      <td>2.422419</td>\n      <td>2.165154</td>\n      <td>2.679630</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.512811</td>\n      <td>3.306587</td>\n      <td>3.550534</td>\n      <td>3.526885</td>\n      <td>3.379601</td>\n      <td>3.199553</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "733ad208ceda4ee58cf6acadacaf02b1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f0eb52870f14c81a2b303603905da6f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10be23cace2142d48f71531ed0e187cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e68b3aa8d8c4364a1756b10da351482"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d312d9b6caf44c21bfab84c3c8f1c988"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53d3dfca5f1640bcb9617a336358a0f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a76fa6f4362a492781e0ffa17e4ddd70"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87a2e6ac535a4ddaa60c03daf2a2179b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8338b1cce4f243648666c46ea356598d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "845602376256400cb4aa095e27044822"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a8e1eb5839d41318ca4208fffe6fd2c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.892292  2.764567    3.033052     2.924136  2.731107   \n1  000BAD50D026  2.634624  2.440516    2.727708     2.377986  2.198006   \n2  00367BB2546B  3.474135  3.367373    3.552581     3.512990  3.378452   \n\n   conventions  \n0     2.655365  \n1     2.607657  \n2     3.297749  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.892292</td>\n      <td>2.764567</td>\n      <td>3.033052</td>\n      <td>2.924136</td>\n      <td>2.731107</td>\n      <td>2.655365</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.634624</td>\n      <td>2.440516</td>\n      <td>2.727708</td>\n      <td>2.377986</td>\n      <td>2.198006</td>\n      <td>2.607657</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.474135</td>\n      <td>3.367373</td>\n      <td>3.552581</td>\n      <td>3.512990</td>\n      <td>3.378452</td>\n      <td>3.297749</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1562c1b86bcf465e86e99f306dec2795"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82c3508aeeff451eb35f74bce467a130"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6a0c65068b84c4381c46bea217729bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2582687a8d0645048e36a4487ca282a7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a22be1915427428ab0a3bca209f78119"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8d96189cdce47a19553741a632dfd7a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d6d46fc82144edc9dc2c664771b9056"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a3bec3611ef4695a3af3243877d0232"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac5034a5fe534014af878f87e05da7c0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38d96291d5de404788047cb936481690"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e5a917947ba4766884951e4193aa9ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.889106  2.760818    3.033244     2.926443  2.730445   \n1  000BAD50D026  2.628500  2.431285    2.723917     2.376415  2.201954   \n2  00367BB2546B  3.462257  3.351451    3.533688     3.494831  3.363663   \n\n   conventions  \n0     2.657448  \n1     2.603708  \n2     3.283415  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.889106</td>\n      <td>2.760818</td>\n      <td>3.033244</td>\n      <td>2.926443</td>\n      <td>2.730445</td>\n      <td>2.657448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.628500</td>\n      <td>2.431285</td>\n      <td>2.723917</td>\n      <td>2.376415</td>\n      <td>2.201954</td>\n      <td>2.603708</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.462257</td>\n      <td>3.351451</td>\n      <td>3.533688</td>\n      <td>3.494831</td>\n      <td>3.363663</td>\n      <td>3.283415</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95478ee242f14828ac8d3e78526e9a9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e8a6e62061f4637aa929c8c74fdaa78"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b95d5c814b24096b60d9b468f834c15"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5a97c6f00c14dae8fe4fb8ebdec2681"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfe42a14776749198beddf83bb1fe525"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07a4275bd45d4431ad7a2c3c88ae11dd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8bf48ed7fcb4106ac8725a6421d3d45"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41c219718d064ca0b4999909a514d985"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f6294799e3c4c82a3ca00c539ee86dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6362156755ad48dba1b2e2f04ff7e906"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee08756592ba4789aa045c4ddf4d242d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.821333  2.717752    2.960445     2.905563  2.694906   \n1  000BAD50D026  2.757675  2.561463    2.745140     2.465197  2.213324   \n2  00367BB2546B  3.507787  3.350252    3.605500     3.552052  3.427625   \n\n   conventions  \n0     2.639460  \n1     2.758724  \n2     3.342166  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.821333</td>\n      <td>2.717752</td>\n      <td>2.960445</td>\n      <td>2.905563</td>\n      <td>2.694906</td>\n      <td>2.639460</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.757675</td>\n      <td>2.561463</td>\n      <td>2.745140</td>\n      <td>2.465197</td>\n      <td>2.213324</td>\n      <td>2.758724</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.507787</td>\n      <td>3.350252</td>\n      <td>3.605500</td>\n      <td>3.552052</td>\n      <td>3.427625</td>\n      <td>3.342166</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c7cef1291f54450a4371e5fa0ceb146"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "047643602bab4d6ea8987332935ca522"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7056b9e8b87341688e13c368aa4537ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1666f77522704ca8bcd2ae1505b57237"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b79d5e8a835a419592015a5c50dccf1a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98b65a63a6aa48b5a9f81f9aebbb3935"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc2e43ad7aad4d4e9e6e0845c7441db9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b6c981edaa470b80d34269527c4e6a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "871a9c73fcb2443b98a64d2f39afe807"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ec632128d144251a97280a155e3792a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec5a6eb86b1b4ee495b0f8a0598391d6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.945490  2.733668    3.020585     2.929958  2.711853   \n1  000BAD50D026  2.757414  2.563560    2.786097     2.492231  2.280460   \n2  00367BB2546B  3.633416  3.379511    3.617248     3.536336  3.321223   \n\n   conventions  \n0     2.664115  \n1     2.686676  \n2     3.336801  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.945490</td>\n      <td>2.733668</td>\n      <td>3.020585</td>\n      <td>2.929958</td>\n      <td>2.711853</td>\n      <td>2.664115</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.757414</td>\n      <td>2.563560</td>\n      <td>2.786097</td>\n      <td>2.492231</td>\n      <td>2.280460</td>\n      <td>2.686676</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.633416</td>\n      <td>3.379511</td>\n      <td>3.617248</td>\n      <td>3.536336</td>\n      <td>3.321223</td>\n      <td>3.336801</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Two Stage Model: Pretrained & FineTuned Model with cuML SVR Regression Head \"\"\"\n",
    "dftr = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\")\n",
    "dftr[\"src\"]=\"train\"\n",
    "dfte = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/test.csv\")\n",
    "dfte[\"src\"]=\"test\"\n",
    "print('Train shape:',dftr.shape,'Test shape:',dfte.shape,'Test columns:',dfte.columns)\n",
    "df = pd.concat([dftr,dfte],ignore_index=True)\n",
    "target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "dftr.head()\n",
    "FOLDS = 25\n",
    "skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for i,(train_index, val_index) in enumerate(skf.split(dftr,dftr[target_cols])):\n",
    "    dftr.loc[val_index,'FOLD'] = i\n",
    "print('Train samples per fold:')\n",
    "dftr.FOLD.value_counts()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state.detach().cpu()\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "# WeightedLayerPooling: Use Intermediate Layer's Embedding\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    For Weighted Layer Pooling Class\n",
    "    In Original Paper, they use [CLS] token for classification task.\n",
    "    But in common sense, Mean Pooling more good performance than CLS token Pooling\n",
    "    So, we append last part of this Pooling Method, Mean Pooling Embedding instad of Using CLS Token\n",
    "    Args:\n",
    "        auto_cfg: AutoConfig from model class member variable\n",
    "        layer_start: how many layers do you want to use, default 21 (last 4 layers)\n",
    "        layer_weights: layer weights for pooling, default None\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_cfg, layer_start: int = 12, layer_weights=None) -> None:\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = auto_cfg.num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (self.num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states, attention_mask) -> Tensor:\n",
    "        all_layer_embedding = torch.stack(list(all_hidden_states), dim=0)\n",
    "        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :].detach().cpu()\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(weighted_average.size()).float()\n",
    "        sum_embeddings = torch.sum(weighted_average * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)  # if lower than thres, replace value to threshold (parameter min)\n",
    "        weighted_mean_embeddings = sum_embeddings / sum_mask\n",
    "        return weighted_mean_embeddings\n",
    "\n",
    "# Attention pooling\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    [Reference]\n",
    "    <A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING>\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "           nn.Linear(auto_cfg.hidden_size, auto_cfg.hidden_size),\n",
    "           nn.LayerNorm(auto_cfg.hidden_size),\n",
    "           nn.GELU(),\n",
    "           nn.Linear(auto_cfg.hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask) -> Tensor:\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask == 0] = float('-inf')\n",
    "        w = torch.softmax(w, 1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "# Mean Pooling\n",
    "class GEMPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized Mean Pooling for Natural Language Processing\n",
    "    This class version of GEMPooling for NLP, Transfer from Computer Vision Task Code\n",
    "\n",
    "    Mean Pooling <= GEMPooling <= Max Pooling\n",
    "    Because of doing exponent to each token embeddings, GEMPooling is like as weight to more activation token\n",
    "\n",
    "    In original paper, they use p=3, but in this class, we use p=4 because torch doesn't support pow calculation\n",
    "    for negative value tensor, only for non-negative value in odd number exponent\n",
    "    [Reference]\n",
    "    https://paperswithcode.com/method/generalized-mean-pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_cfg) -> None:\n",
    "        super(GEMPooling, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(last_hidden_state, attention_mask, p: int = 4) -> Tensor:\n",
    "        \"\"\"\n",
    "        1) Expand Attention Mask from [batch_size, max_len] to [batch_size, max_len, hidden_size]\n",
    "        2) Sum Embeddings along max_len axis so now we have [batch_size, hidden_size]\n",
    "        3) Sum Mask along max_len axis, This is done so that we can ignore padding tokens\n",
    "        4) Average\n",
    "        \"\"\"\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(\n",
    "            torch.pow(last_hidden_state * input_mask_expanded, p), 1\n",
    "        )\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        tmp_embeddings = sum_embeddings / sum_mask\n",
    "        gem_embeddings = torch.pow(tmp_embeddings, 1/p)\n",
    "        return gem_embeddings\n",
    "    \n",
    "BATCH_SIZE = 4\n",
    "\n",
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self,idx):\n",
    "        text = self.df.loc[idx,\"full_text\"]\n",
    "        tokens = tokenizer(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,return_tensors=\"pt\")\n",
    "        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "ds_tr = EmbedDataset(dftr)\n",
    "embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)\n",
    "ds_te = EmbedDataset(dfte)\n",
    "embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)\n",
    "\n",
    "train_config = False\n",
    "tokenizer = None\n",
    "MAX_LEN = 640\n",
    "\n",
    "def meanpooling_get_embeddings(finetuned_weight=None, MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN, train_config\n",
    "    DEVICE=\"cuda\"\n",
    "    auto_cfg = AutoConfig.from_pretrained(MODEL_NM, output_hidden_states=True)\n",
    "    model = AutoModel.from_pretrained(MODEL_NM, config=auto_cfg)\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    MAX_LEN = MAX\n",
    "    if finetuned_weight is not None:\n",
    "        model.load_state_dict(torch.load(finetuned_weight, map_location='cuda:0'), strict=False)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if train_config:\n",
    "        all_train_text_feats = []\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Good\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_train_text_feats.extend(sentence_embeddings)\n",
    "        all_train_text_feats = np.array(all_train_text_feats)\n",
    "        if verbose:\n",
    "            print('Train embeddings shape',all_train_text_feats.shape)\n",
    "        \n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return te_text_feats\n",
    "\n",
    "def weightedlayer_get_embeddings(finetuned_weight=None, MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN, train_config\n",
    "    DEVICE=\"cuda\"\n",
    "    auto_cfg = AutoConfig.from_pretrained(MODEL_NM, output_hidden_states=True)\n",
    "    model = AutoModel.from_pretrained(MODEL_NM, config=auto_cfg)\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    WeightedLayerPool = WeightedLayerPooling(auto_cfg)\n",
    "    MAX_LEN = MAX\n",
    "    \n",
    "    if finetuned_weight is not None:\n",
    "        model.load_state_dict(torch.load(finetuned_weight, map_location='cuda:0'), strict=False)\n",
    "   \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if train_config:\n",
    "        all_train_text_feats = []\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = WeightedLayerPool(\n",
    "                model_output.hidden_states,\n",
    "                attention_mask.detach().cpu()\n",
    "            )\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Good\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_train_text_feats.extend(sentence_embeddings)\n",
    "        all_train_text_feats = np.array(all_train_text_feats)\n",
    "        if verbose:\n",
    "            print('Train embeddings shape',all_train_text_feats.shape)\n",
    "        \n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = WeightedLayerPool(\n",
    "            model_output.hidden_states,\n",
    "            attention_mask.detach().cpu()\n",
    "        )\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return te_text_feats\n",
    "\n",
    "def four_weightedlayer_get_embeddings(finetuned_weight=None, MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN, train_config\n",
    "    DEVICE=\"cuda\"\n",
    "    auto_cfg = AutoConfig.from_pretrained(MODEL_NM, output_hidden_states=True)\n",
    "    model = AutoModel.from_pretrained(MODEL_NM, config=auto_cfg)\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    WeightedLayerPool = WeightedLayerPooling(auto_cfg, layer_start=4)\n",
    "    MAX_LEN = MAX\n",
    "    \n",
    "    if finetuned_weight is not None:\n",
    "        model.load_state_dict(torch.load(finetuned_weight, map_location='cuda:0'), strict=False)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if train_config: \n",
    "        all_train_text_feats = []\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = WeightedLayerPool(\n",
    "                model_output.hidden_states,\n",
    "                attention_mask.detach().cpu()\n",
    "            )\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Good\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_train_text_feats.extend(sentence_embeddings)\n",
    "        all_train_text_feats = np.array(all_train_text_feats)\n",
    "        if verbose:\n",
    "            print('Train embeddings shape',all_train_text_feats.shape)\n",
    "        \n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = WeightedLayerPool(\n",
    "            model_output.hidden_states,\n",
    "            attention_mask.detach().cpu()\n",
    "        )\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return te_text_feats\n",
    "\n",
    "def gempooling_get_embeddings(finetuned_weight=None, MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN, train_config\n",
    "    DEVICE=\"cuda\"\n",
    "    auto_cfg = AutoConfig.from_pretrained(MODEL_NM, output_hidden_states=True)\n",
    "    model = AutoModel.from_pretrained(MODEL_NM, config=auto_cfg)\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    GEMPool = GEMPooling(auto_cfg)\n",
    "    MAX_LEN = MAX\n",
    "    \n",
    "    if finetuned_weight is not None:\n",
    "        model.load_state_dict(torch.load(finetuned_weight, map_location='cuda:0'), strict=False)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if train_config:\n",
    "        all_train_text_feats = []\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = GEMPool(\n",
    "                model_output.last_hidden_state.detach().cpu(),\n",
    "                attention_mask.detach().cpu()\n",
    "            )\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Good\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_train_text_feats.extend(sentence_embeddings)\n",
    "        all_train_text_feats = np.array(all_train_text_feats)\n",
    "        if verbose:\n",
    "            print('Train embeddings shape',all_train_text_feats.shape)\n",
    "        \n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = GEMPool(\n",
    "            model_output.last_hidden_state.detach().cpu(),\n",
    "            attention_mask.detach().cpu()\n",
    "        )        \n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return te_text_feats\n",
    "\n",
    "def attentionpooling_get_embeddings(finetuned_weight=None, MODEL_NM='', MAX=640, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, MAX_LEN, train_config\n",
    "    DEVICE=\"cuda\"\n",
    "    auto_cfg = AutoConfig.from_pretrained(MODEL_NM, output_hidden_states=True)\n",
    "    model = AutoModel.from_pretrained(MODEL_NM, config=auto_cfg)\n",
    "    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n",
    "    AttentionPool = AttentionPooling(auto_cfg)\n",
    "    MAX_LEN = MAX\n",
    "    \n",
    "    if finetuned_weight is not None:\n",
    "        model.load_state_dict(torch.load(finetuned_weight, map_location='cuda:0'), strict=False)\n",
    "   \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    if train_config:\n",
    "        all_train_text_feats = []\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            sentence_embeddings = AttentionPool(\n",
    "                model_output.last_hidden_state.detach().cpu(),\n",
    "                attention_mask.detach().cpu()\n",
    "            )\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1) # Good\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_train_text_feats.extend(sentence_embeddings)\n",
    "        all_train_text_feats = np.array(all_train_text_feats)\n",
    "        if verbose:\n",
    "            print('Train embeddings shape',all_train_text_feats.shape)\n",
    "\n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = AttentionPool(\n",
    "            model_output.last_hidden_state.detach().cpu(),\n",
    "            attention_mask.detach().cpu()\n",
    "        )\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    return te_text_feats\n",
    "\n",
    "MODEL_NM = '../input/deberta-v3-large/deberta-v3-large'\n",
    "class FineTunedModel:\n",
    "    MeanPool_Model = glob.glob('/kaggle/input/0927-deberta-v3-large-unscale/*.pth')\n",
    "    GEMPool_Model = glob.glob('/kaggle/input/fbp3-gempooling-max-len-1536-04476/*.pth')\n",
    "    WeightedLayer_Model = glob.glob('/kaggle/input/fbp3-weightedlayerpooling-04545/*.pth')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T15:46:37.438721Z",
     "iopub.execute_input": "2023-04-23T15:46:37.439703Z",
     "iopub.status.idle": "2023-04-23T15:46:37.871099Z",
     "shell.execute_reply.started": "2023-04-23T15:46:37.439664Z",
     "shell.execute_reply": "2023-04-23T15:46:37.869967Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train shape: (3911, 9) Test shape: (3, 3) Test columns: Index(['text_id', 'full_text', 'src'], dtype='object')\nTrain samples per fold:\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Train Stage \"\"\"\n",
    "target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "MODEL_NM = '/kaggle/input/huggingface-automodel-save/deberta-v3-large'\n",
    "class FineTunedModel:\n",
    "    MeanPool_Model = glob.glob('/kaggle/input/0927-deberta-v3-large-unscale/*.pth')\n",
    "    GEMPool_Model = glob.glob('/kaggle/input/fbp3-gempooling-max-len-1536-04476/*.pth')\n",
    "    WeightedLayer_Model = glob.glob('/kaggle/input/fbp3-weightedlayerpooling-04545/*.pth')\n",
    "    \n",
    "te_text_feats1 = meanpooling_get_embeddings(MODEL_NM=MODEL_NM)\n",
    "te_text_feats2 = weightedlayer_get_embeddings(MODEL_NM=MODEL_NM)\n",
    "te_text_feats3 = four_weightedlayer_get_embeddings(MODEL_NM=MODEL_NM)\n",
    "te_text_feats4 = gempooling_get_embeddings(MODEL_NM=MODEL_NM)\n",
    "te_text_feats5 = attentionpooling_get_embeddings(MODEL_NM=MODEL_NM, MAX=512)\n",
    "\n",
    "# all_train_text_feats = np.concatenate([all_train_text_feats,all_train_text_feats2,\n",
    "#                                        all_train_text_feats3,all_train_text_feats4,\n",
    "#                                        all_train_text_feats5],axis=1)\n",
    "\"\"\" Baseline Train Embeddings \"\"\"\n",
    "all_train_text_feats = np.load('/kaggle/input/fbp3-svr-train-embeddings/baseline_train_embedding.npy')\n",
    "te_text_feats = np.concatenate([\n",
    "    te_text_feats1,\n",
    "    te_text_feats2,\n",
    "    te_text_feats3,\n",
    "    te_text_feats4,\n",
    "    te_text_feats5\n",
    "],axis=1)\n",
    "\n",
    "# del all_train_text_feats2, te_text_feats2\n",
    "# del all_train_text_feats3, te_text_feats3\n",
    "# del all_train_text_feats4, te_text_feats4\n",
    "\n",
    "del te_text_feats2\n",
    "del te_text_feats3\n",
    "del te_text_feats4\n",
    "\n",
    "\"\"\" Inference Stage 1\"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    global target_cols\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('#'*25)\n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold]\n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold]\n",
    "    \n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print()\n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))\n",
    "\n",
    "sub = dfte.copy()\n",
    "\n",
    "sub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\n",
    "sub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\n",
    "sub = sub[sub_columns]\n",
    "sub.to_csv(\"submission_15.csv\",index=None)\n",
    "\n",
    "\"\"\" Inference Stage 2 \"\"\"\n",
    "\"\"\" load Fine-Tuned Model at number two variable \"\"\"\n",
    "te_text_feats2 = 0\n",
    "for model in tqdm(FineTunedModel.WeightedLayer_Model):\n",
    "    tmp_te_text_feats = weightedlayer_get_embeddings(\n",
    "        model, \n",
    "        MODEL_NM=MODEL_NM\n",
    "    )\n",
    "    te_text_feats2 += tmp_te_text_feats\n",
    "te_text_feats2 = te_text_feats2 / 5\n",
    "\n",
    "\"\"\" load Fine-Tuned Model at number three variable \"\"\"\n",
    "te_text_feats3 = 0\n",
    "for model in tqdm(FineTunedModel.WeightedLayer_Model):\n",
    "    tmp_te_text_feats = four_weightedlayer_get_embeddings(\n",
    "        model,\n",
    "        MODEL_NM=MODEL_NM\n",
    "    )\n",
    "    te_text_feats3 += tmp_te_text_feats\n",
    "te_text_feats3 = te_text_feats3 / 5\n",
    "\n",
    "\"\"\" load Fine-Tuned Model at number four variable \"\"\"\n",
    "te_text_feats4 = 0\n",
    "for model in tqdm(FineTunedModel.WeightedLayer_Model):\n",
    "    tmp_te_text_feats = gempooling_get_embeddings(\n",
    "        model,\n",
    "        MODEL_NM=MODEL_NM\n",
    "    )\n",
    "    te_text_feats4 += tmp_te_text_feats\n",
    "te_text_feats4 = te_text_feats4 / 5\n",
    "\n",
    "all_train_text_feats = np.load('/kaggle/input/fbp3-svr-train-embeddings/train_embedding.npy')\n",
    "te_text_feats = np.concatenate([\n",
    "    te_text_feats1,\n",
    "    te_text_feats2,\n",
    "    te_text_feats3,\n",
    "    te_text_feats4,\n",
    "    te_text_feats5]\n",
    ", axis=1)\n",
    "\n",
    "\"\"\" Inference Stage 2\"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    global target_cols\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('#'*25)\n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold]\n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold]\n",
    "    \n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print()\n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))\n",
    "\n",
    "sub = dfte.copy()\n",
    "\n",
    "sub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\n",
    "sub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\n",
    "sub = sub[sub_columns]\n",
    "sub.to_csv(\"submission_16.csv\",index=None)\n",
    "\n",
    "del te_text_feats2\n",
    "del te_text_feats3\n",
    "del te_text_feats4\n",
    "gc.collect()\n",
    "\n",
    "print('Our concatenated embeddings have shape', all_train_text_feats.shape )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T15:46:37.873787Z",
     "iopub.execute_input": "2023-04-23T15:46:37.874448Z",
     "iopub.status.idle": "2023-04-23T15:54:06.682515Z",
     "shell.execute_reply.started": "2023-04-23T15:46:37.874408Z",
     "shell.execute_reply": "2023-04-23T15:54:06.681221Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7e9efb90f9b4a0b94240cac7427b1e5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a578262f64bb4c18a6d37b1cf7c486c9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a20f3b69ad04b0babcbd4e1c99e3181"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26b0ba8bf8af4d73a48499f7b373fca0"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b61b7e3182e4c18b086c94097b56efc"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n#########################\n### Fold 1\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 0 RSME score: 0.45455565750401217\n#########################\n### Fold 2\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 1 RSME score: 0.45118534040919106\n#########################\n### Fold 3\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 2 RSME score: 0.46060901547360866\n#########################\n### Fold 4\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 3 RSME score: 0.445561795011782\n#########################\n### Fold 5\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 4 RSME score: 0.4458096112597782\n#########################\n### Fold 6\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 5 RSME score: 0.43692824706622874\n#########################\n### Fold 7\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 6 RSME score: 0.44276123606223555\n#########################\n### Fold 8\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 7 RSME score: 0.4606260283400682\n#########################\n### Fold 9\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 8 RSME score: 0.44521151243010876\n#########################\n### Fold 10\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 9 RSME score: 0.48370475036595445\n#########################\n### Fold 11\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 10 RSME score: 0.4397103783972955\n#########################\n### Fold 12\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 11 RSME score: 0.44150034952554434\n#########################\n### Fold 13\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 12 RSME score: 0.46581385975094153\n#########################\n### Fold 14\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 13 RSME score: 0.43168895650259226\n#########################\n### Fold 15\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 14 RSME score: 0.4594221665105516\n#########################\n### Fold 16\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 15 RSME score: 0.4370354115746871\n#########################\n### Fold 17\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 16 RSME score: 0.42926439826317325\n#########################\n### Fold 18\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 17 RSME score: 0.4595572542788489\n#########################\n### Fold 19\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 18 RSME score: 0.45560768948899194\n#########################\n### Fold 20\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 19 RSME score: 0.4337902199710623\n#########################\n### Fold 21\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 20 RSME score: 0.47171912356902096\n#########################\n### Fold 22\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 21 RSME score: 0.4627580005644789\n#########################\n### Fold 23\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 22 RSME score: 0.4490816708690888\n#########################\n### Fold 24\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 23 RSME score: 0.45283281861884267\n#########################\n### Fold 25\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 24 RSME score: 0.42895733683578063\n#########################\nOverall CV RSME = 0.4498277131457548\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6f9fcba44b54971b811412d3d1b25f0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96ded303dbdf45efb7a87b232d5cebaa"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "192557280ba74edea799c6bb4d96c7b8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3899737326574551a6f9ac42c07e0f8f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2efeb6757c654141af52df3a37b93e8d"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8824fb8fce84681a08ad9eaaa850aa1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b813cfa1a334140ae739f1c58259fee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9f75de0ad33452d84d96da0fe4f10b9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41f92d3a8937425eb769dcabe4bbb805"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "139b1e2391eb4b64984d5c48cd034395"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e682ab81d7854103b3ecedde20df03c6"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91ee2663eb2b439c9034b0a65f95a045"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cf267bda9054e40b7c143af34ac5f43"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aee409bcd5a04daebcfdf83c2e981e8a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "372143280b084f68a688f8fe20627ebd"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34922129706846728a1bd4c014d9192e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad3b06f364ed4527a567d139481123f1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "258bc4fdc10f4fa1ba6cb736568d27d1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Test embeddings shape (3, 1024)\n#########################\n### Fold 1\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 0 RSME score: 0.4546352449487106\n#########################\n### Fold 2\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 1 RSME score: 0.45149397206156044\n#########################\n### Fold 3\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 2 RSME score: 0.4605507540373644\n#########################\n### Fold 4\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 3 RSME score: 0.44575200847249546\n#########################\n### Fold 5\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 4 RSME score: 0.44576194994566976\n#########################\n### Fold 6\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 5 RSME score: 0.4367717438362415\n#########################\n### Fold 7\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 6 RSME score: 0.44292412390515695\n#########################\n### Fold 8\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 7 RSME score: 0.46105548826399984\n#########################\n### Fold 9\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 8 RSME score: 0.4456100945482803\n#########################\n### Fold 10\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 9 RSME score: 0.48387357407755816\n#########################\n### Fold 11\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 10 RSME score: 0.4394740004411903\n#########################\n### Fold 12\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 11 RSME score: 0.44175099319235195\n#########################\n### Fold 13\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 12 RSME score: 0.4656611768615268\n#########################\n### Fold 14\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 13 RSME score: 0.431717353175107\n#########################\n### Fold 15\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 14 RSME score: 0.4594798001851816\n#########################\n### Fold 16\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 15 RSME score: 0.43705255308404195\n#########################\n### Fold 17\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 16 RSME score: 0.4294654294902274\n#########################\n### Fold 18\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 17 RSME score: 0.45962087856006467\n#########################\n### Fold 19\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 18 RSME score: 0.45597158582745406\n#########################\n### Fold 20\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 19 RSME score: 0.433898189221178\n#########################\n### Fold 21\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 20 RSME score: 0.47152302749582725\n#########################\n### Fold 22\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 21 RSME score: 0.46265220527418066\n#########################\n### Fold 23\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 22 RSME score: 0.448684057068039\n#########################\n### Fold 24\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 23 RSME score: 0.45284713031378176\n#########################\n### Fold 25\n#########################\ncohesion , syntax , vocabulary , phraseology , grammar , conventions , \nFold : 24 RSME score: 0.42872555691358244\n#########################\nOverall CV RSME = 0.4498781156480309\nOur concatenated embeddings have shape (3911, 5120)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\" Inference Stage \"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = []\n",
    "scores = []\n",
    "def comp_score(y_true,y_pred):\n",
    "    rmse_scores = []\n",
    "    for i in range(len(target_cols)):\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "#for fold in tqdm(range(FOLDS),total=FOLDS):\n",
    "for fold in range(FOLDS):\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('#'*25)\n",
    "    \n",
    "    dftr_ = dftr[dftr[\"FOLD\"]!=fold]\n",
    "    dfev_ = dftr[dftr[\"FOLD\"]==fold]\n",
    "    \n",
    "    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n",
    "    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n",
    "    \n",
    "    ev_preds = np.zeros((len(ev_text_feats),6))\n",
    "    test_preds = np.zeros((len(te_text_feats),6))\n",
    "    for i,t in enumerate(target_cols):\n",
    "        print(t,', ',end='')\n",
    "        clf = SVR(C=1)\n",
    "        clf.fit(tr_text_feats, dftr_[t].values)\n",
    "        ev_preds[:,i] = clf.predict(ev_text_feats)\n",
    "        test_preds[:,i] = clf.predict(te_text_feats)\n",
    "    print()\n",
    "    score = comp_score(dfev_[target_cols].values,ev_preds)\n",
    "    scores.append(score)\n",
    "    print(\"Fold : {} RSME score: {}\".format(fold,score))\n",
    "    preds.append(test_preds)\n",
    "    \n",
    "print('#'*25)\n",
    "print('Overall CV RSME =',np.mean(scores))\n",
    "\n",
    "sub = dfte.copy()\n",
    "\n",
    "sub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\n",
    "sub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\n",
    "sub = sub[sub_columns]\n",
    "sub.to_csv(\"submission_9.csv\",index=None)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
    "submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n",
    "\n",
    "sub1 = pd.read_csv(f'submission_1.csv')[CFG1.target_cols] * CFG1.weight\n",
    "sub2 = pd.read_csv(f'submission_2.csv')[CFG2.target_cols] * CFG2.weight\n",
    "sub3 = pd.read_csv(f'submission_3.csv')[CFG3.target_cols] * CFG3.weight\n",
    "sub4 = pd.read_csv(f'submission_4.csv')[CFG4.target_cols] * CFG4.weight\n",
    "sub5 = pd.read_csv(f'submission_5.csv')[CFG5.target_cols] * CFG5.weight\n",
    "sub6 = pd.read_csv(f'submission_6.csv')[CFG6.target_cols] * CFG6.weight\n",
    "sub7 = pd.read_csv(f'submission_7.csv')[CFG7.target_cols] * CFG7.weight\n",
    "sub8 = pd.read_csv(f'submission_8.csv')[CFG8.target_cols] * CFG8.weight\n",
    "sub9 = pd.read_csv(f'submission_9.csv')[CFG9.target_cols] * CFG9.weight\n",
    "sub10 = pd.read_csv(f'submission_10.csv')[CFG10.target_cols] * CFG10.weight\n",
    "sub11 = pd.read_csv(f'submission_11.csv')[CFG11.target_cols] * CFG11.weight\n",
    "sub12 = pd.read_csv(f'submission_12.csv')[CFG12.target_cols] * CFG12.weight\n",
    "sub13 = pd.read_csv(f'submission_13.csv')[CFG13.target_cols] * CFG13.weight\n",
    "sub14 = pd.read_csv(f'submission_14.csv')[CFG14.target_cols] * CFG14.weight\n",
    "# sub15 = pd.read_csv(f'submission_15.csv')[CFG1.target_cols] * 1.0\n",
    "# sub16 = pd.read_csv(f'submission_16.csv')[CFG1.target_cols] * 1.0\n",
    "\n",
    "\n",
    "# ens = (sub1 + sub2 + sub3 + sub4 + sub5 + sub6 + sub7 + sub8 + sub9 + sub10 + sub11 + sub12 + sub13 + sub14 + sub15 + sub16)/(CFG1.weight + CFG2.weight + CFG3.weight + CFG4.weight + CFG5.weight + CFG6.weight + CFG7.weight + CFG8.weight + CFG9.weight + CFG10.weight + CFG11.weight + CFG12.weight + CFG13.weight + CFG14.weight + 1.0 + 1.0)\n",
    "ens = (sub1 + sub2 + sub3 + sub4 + sub5 + sub6 + sub7 + sub8 + sub9 + sub10 + sub11 + sub12 + sub13 + sub14)/(CFG1.weight + CFG2.weight + CFG3.weight + CFG4.weight + CFG5.weight + CFG6.weight + CFG7.weight + CFG8.weight + CFG9.weight + CFG10.weight + CFG11.weight + CFG12.weight + CFG13.weight + CFG14.weight)\n",
    "\n",
    "submission[CFG1.target_cols] = ens\n",
    "display(submission.head())\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-23T15:54:06.686779Z",
     "iopub.execute_input": "2023-04-23T15:54:06.689032Z",
     "iopub.status.idle": "2023-04-23T15:54:06.783387Z",
     "shell.execute_reply.started": "2023-04-23T15:54:06.688994Z",
     "shell.execute_reply": "2023-04-23T15:54:06.782385Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  2.890979  2.758132    3.062877     2.927794  2.689081   \n1  000BAD50D026  2.664568  2.477626    2.723253     2.404711  2.162099   \n2  00367BB2546B  3.535244  3.381122    3.574189     3.567170  3.406775   \n\n   conventions  \n0     2.664978  \n1     2.641702  \n2     3.299109  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>2.890979</td>\n      <td>2.758132</td>\n      <td>3.062877</td>\n      <td>2.927794</td>\n      <td>2.689081</td>\n      <td>2.664978</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>2.664568</td>\n      <td>2.477626</td>\n      <td>2.723253</td>\n      <td>2.404711</td>\n      <td>2.162099</td>\n      <td>2.641702</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.535244</td>\n      <td>3.381122</td>\n      <td>3.574189</td>\n      <td>3.567170</td>\n      <td>3.406775</td>\n      <td>3.299109</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \"\"\" Result Weight Tuning \"\"\"\n",
    "# # Final_Score = (W1 * R1) + (W2 * R2) + (W3 * R3) + (W4 * R4) + (W5 * R5)\n",
    "# final_submission = sample_submission_df.copy()\n",
    "# CFG1_submission = pd.read_csv('/kaggle/working/CFG1_submission.csv')\n",
    "# CFG2_submission = pd.read_csv('/kaggle/working/CFG2_submission.csv')\n",
    "# #CFG3_submission = pd.read_csv('/kaggle/working/CFG3_submission.csv')\n",
    "# #CFG4_submission = pd.read_csv('/kaggle/working/CFG4_submission.csv')\n",
    "# #CFG5_submission = pd.read_csv('/kaggle/working/CFG5_submission.csv')\n",
    "\n",
    "# result_list = [\n",
    "#     CFG1_submission,\n",
    "#     CFG2_submission,\n",
    "# #    CFG3_submission,\n",
    "# #    CFG4_submission,\n",
    "# #    CFG5_submission\n",
    "# ]\n",
    "\n",
    "# final_predictions = np.zeros((len(sample_submission_df), len(sample_submission_df.iloc[0,1:])))\n",
    "# for result in result_list:\n",
    "#     tmp_predictions = np.array(result.iloc[:, 1:7])\n",
    "#     final_predictions = np.add(final_predictions, tmp_predictions)\n",
    "# final_predictions = final_predictions * 1/len(result_list)\n",
    "# final_submission[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']] = final_predictions\n",
    "# final_submission.reset_index(drop=True)\n",
    "# final_submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-17T07:50:33.850838Z",
     "iopub.execute_input": "2023-04-17T07:50:33.851688Z",
     "iopub.status.idle": "2023-04-17T07:50:33.871079Z",
     "shell.execute_reply.started": "2023-04-17T07:50:33.851646Z",
     "shell.execute_reply": "2023-04-17T07:50:33.870033Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
